---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-"
)
```

# Lexicon-based Sentiment Analysis for Economic and Financial Applications

The `FiGASR` package allows R users to leverage on cutting-hedge NLP techniques to easily run sentiment analysis on economic news content:
this package is a wrapper of the [`SentiBigNomics`](https://github.com/sergioconsoli/SentiBigNomics) python package.
Given a list of texts as input and a list of tokens of interest (ToI), the algorithm analyses the texts and compute the economic sentiment associated each ToI. 
Two key features characterize this approach.
First, it is *fine-grained*, since words are assigned a polarity score
that ranges in [-1,1] based on a dictionary.
Second, it is *aspect-based*, since the algorithm selects the chunk of text that relates to the ToI based on a set of semantic rules and calculates the sentiment only on that text, rather than the full article. 

The package includes some additional of features, like automatic negation handling, tense detection, location filtering and excluding some words from the sentiment computation.
`FiGASR` only supports English language, as it relies on the *en_core_web_lg* language model from the `spaCy` Python module.




## Installation

You can install the package from GitHub as follows:

```{r gh-installation, eval = FALSE}
install.packages("devtools")
devtools::install_github("lucabarbaglia/FiGASR")
```

If it is the first time that you are using `FiGASR`, then set up the associated environment:

```{r env, eval = FALSE}
FiGASR::figas_install()
```


## A start-up example

Let's assume that you want to compute the sentiment associated to two tokens of interest, namely *unemployment* and *economy*, given the two following sentences.

```{r test}
library(FiGASR)
text <- list("Unemployment is rising at high speed",
             "The economy is slowing down and unemployment is booming")
include = list("unemployment", "economy")

get_sentiment(text = text, include = include)

```

The output of the function `get_sentiment` is a list, containing two objects:

- a tibble "sentiment" containing the average sentiment computed for each text;

- a tibble "sentiment_by_chunk" containing the sentiment computed for each chunk detected in the texts.

The first element of the output list provides the overall average sentiment score of each text, while the second provides the detailed score of each chunk of text that relates to one of the ToI.




## Citation:

If you use this package, please *cite* the following references:

<!-- ## References: -->

* Consoli, Barbaglia, Manzan (January 14, 2021). Fine-Grained  Aspect-Based Sentiment Analysis on Economic and Financial Lexicon. Available at SSRN: https://ssrn.com/abstract=3766194

* Barbaglia,  Consoli, Manzan (September 23, 2020). Forecasting with Economic News. Available at SSRN: https://ssrn.com/abstract=3698121

* Barbaglia, Consoli, Manzan (2020). Monitoring the Business Cycle with Fine-Grained, Aspect-Based Sentiment Extraction from News. In: Bitetta, Bordino, Ferretti, Gullo, Pascolutti, Ponti (eds) Mining Data for Financial Applications. MIDAS 2019. Lecture Notes in Computer Science, vol 11985. Springer.

## Notes:
